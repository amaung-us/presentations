{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56ad8e79-9279-411e-91a2-173aff02c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlxModelPath = \"../Models/mlx_models/8b-Instruct\"\n",
    "hf_converted_model_path=\"../Models/hf_converted_models/8b-Instruct\"\n",
    "\n",
    "newMLXModelPath = \"../Models/mlx_models/8b-Instruct-FineTuned\"\n",
    "dataPath=\"../Data/Jsonl\"\n",
    "iterations=500\n",
    "max_length=2049\n",
    "adapter_path=\"../Models/FineTune/8b-Instruct/Adapters\"\n",
    "savePath = \"../Models/saved-model/kcdc-8b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ac4840-53bd-413a-9db2-bb10e396927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import lora, generate, load, stream_generate\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef792f7f-c290-44cb-bc83-0d40a1817874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FineTune\n",
      "python3 -m mlx_lm.lora --model ../Models/mlx_models/8b-Instruct --data ../Data/Jsonl --train --iters 500 --max-seq-length 2049 --adapter-path ../Models/FineTune/8b-Instruct/Adapters\n",
      "/Users/##USER##/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Loading pretrained model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.021% (1.704M/8030.261M)\n",
      "Starting training..., iters: 500\n",
      "Iter 1: Val loss 2.796, Val took 4.489s\n",
      "Iter 10: Train loss 2.809, Learning Rate 1.000e-05, It/sec 0.383, Tokens/sec 289.735, Trained Tokens 7570, Peak mem 10.996 GB\n",
      "Iter 20: Train loss 2.835, Learning Rate 1.000e-05, It/sec 0.396, Tokens/sec 272.691, Trained Tokens 14460, Peak mem 10.996 GB\n",
      "Iter 30: Train loss 2.405, Learning Rate 1.000e-05, It/sec 0.351, Tokens/sec 266.056, Trained Tokens 22030, Peak mem 10.996 GB\n",
      "Iter 40: Train loss 2.129, Learning Rate 1.000e-05, It/sec 0.356, Tokens/sec 269.772, Trained Tokens 29600, Peak mem 10.996 GB\n",
      "Iter 50: Train loss 1.900, Learning Rate 1.000e-05, It/sec 0.371, Tokens/sec 272.282, Trained Tokens 36944, Peak mem 10.996 GB\n",
      "Iter 60: Train loss 1.541, Learning Rate 1.000e-05, It/sec 0.376, Tokens/sec 267.835, Trained Tokens 44060, Peak mem 10.996 GB\n",
      "Iter 70: Train loss 1.163, Learning Rate 1.000e-05, It/sec 0.377, Tokens/sec 267.954, Trained Tokens 51176, Peak mem 10.996 GB\n",
      "Iter 80: Train loss 0.879, Learning Rate 1.000e-05, It/sec 0.345, Tokens/sec 268.792, Trained Tokens 58974, Peak mem 10.996 GB\n",
      "Iter 90: Train loss 0.551, Learning Rate 1.000e-05, It/sec 0.370, Tokens/sec 263.331, Trained Tokens 66090, Peak mem 10.996 GB\n",
      "Iter 100: Train loss 0.383, Learning Rate 1.000e-05, It/sec 0.333, Tokens/sec 251.925, Trained Tokens 73660, Peak mem 10.996 GB\n",
      "Iter 100: Saved adapter weights to ../Models/FineTune/8b-Instruct/Adapters/adapters.safetensors and ../Models/FineTune/8b-Instruct/Adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.255, Learning Rate 1.000e-05, It/sec 0.332, Tokens/sec 228.814, Trained Tokens 80550, Peak mem 10.996 GB\n",
      "Iter 120: Train loss 0.175, Learning Rate 1.000e-05, It/sec 0.265, Tokens/sec 200.439, Trained Tokens 88120, Peak mem 10.996 GB\n",
      "Iter 130: Train loss 0.122, Learning Rate 1.000e-05, It/sec 0.226, Tokens/sec 171.295, Trained Tokens 95690, Peak mem 10.996 GB\n",
      "Iter 140: Train loss 0.099, Learning Rate 1.000e-05, It/sec 0.228, Tokens/sec 167.569, Trained Tokens 103034, Peak mem 10.996 GB\n",
      "Iter 150: Train loss 0.088, Learning Rate 1.000e-05, It/sec 0.264, Tokens/sec 188.169, Trained Tokens 110150, Peak mem 10.996 GB\n",
      "Iter 160: Train loss 0.081, Learning Rate 1.000e-05, It/sec 0.290, Tokens/sec 206.256, Trained Tokens 117266, Peak mem 10.996 GB\n",
      "Iter 170: Train loss 0.071, Learning Rate 1.000e-05, It/sec 0.281, Tokens/sec 219.324, Trained Tokens 125064, Peak mem 10.996 GB\n",
      "Iter 180: Train loss 0.072, Learning Rate 1.000e-05, It/sec 0.320, Tokens/sec 227.901, Trained Tokens 132180, Peak mem 10.996 GB\n",
      "Iter 190: Train loss 0.066, Learning Rate 1.000e-05, It/sec 0.320, Tokens/sec 234.950, Trained Tokens 139524, Peak mem 10.996 GB\n",
      "Iter 200: Val loss 0.053, Val took 5.191s\n",
      "Iter 200: Train loss 0.061, Learning Rate 1.000e-05, It/sec 3.256, Tokens/sec 2317.059, Trained Tokens 146640, Peak mem 10.996 GB\n",
      "Iter 200: Saved adapter weights to ../Models/FineTune/8b-Instruct/Adapters/adapters.safetensors and ../Models/FineTune/8b-Instruct/Adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.049, Learning Rate 1.000e-05, It/sec 0.324, Tokens/sec 245.268, Trained Tokens 154210, Peak mem 10.996 GB\n",
      "Iter 220: Train loss 0.039, Learning Rate 1.000e-05, It/sec 0.328, Tokens/sec 241.236, Trained Tokens 161554, Peak mem 10.996 GB\n",
      "Iter 230: Train loss 0.029, Learning Rate 1.000e-05, It/sec 0.387, Tokens/sec 275.673, Trained Tokens 168670, Peak mem 10.996 GB\n",
      "Iter 240: Train loss 0.022, Learning Rate 1.000e-05, It/sec 0.355, Tokens/sec 268.399, Trained Tokens 176240, Peak mem 10.996 GB\n",
      "Iter 250: Train loss 0.020, Learning Rate 1.000e-05, It/sec 0.357, Tokens/sec 262.395, Trained Tokens 183584, Peak mem 10.996 GB\n",
      "Iter 260: Train loss 0.019, Learning Rate 1.000e-05, It/sec 0.351, Tokens/sec 257.667, Trained Tokens 190926, Peak mem 10.996 GB\n",
      "Iter 270: Train loss 0.019, Learning Rate 1.000e-05, It/sec 0.357, Tokens/sec 262.113, Trained Tokens 198270, Peak mem 10.996 GB\n",
      "Iter 280: Train loss 0.018, Learning Rate 1.000e-05, It/sec 0.337, Tokens/sec 255.043, Trained Tokens 205840, Peak mem 10.996 GB\n",
      "Iter 290: Train loss 0.018, Learning Rate 1.000e-05, It/sec 0.324, Tokens/sec 230.548, Trained Tokens 212956, Peak mem 10.996 GB\n",
      "Iter 300: Train loss 0.018, Learning Rate 1.000e-05, It/sec 0.259, Tokens/sec 190.415, Trained Tokens 220300, Peak mem 10.996 GB\n",
      "Iter 300: Saved adapter weights to ../Models/FineTune/8b-Instruct/Adapters/adapters.safetensors and ../Models/FineTune/8b-Instruct/Adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.018, Learning Rate 1.000e-05, It/sec 0.192, Tokens/sec 136.700, Trained Tokens 227416, Peak mem 10.996 GB\n",
      "Iter 320: Train loss 0.016, Learning Rate 1.000e-05, It/sec 0.149, Tokens/sec 115.808, Trained Tokens 235214, Peak mem 10.996 GB\n",
      "Iter 330: Train loss 0.018, Learning Rate 1.000e-05, It/sec 0.239, Tokens/sec 169.823, Trained Tokens 242330, Peak mem 10.996 GB\n",
      "Iter 340: Train loss 0.018, Learning Rate 1.000e-05, It/sec 0.272, Tokens/sec 193.498, Trained Tokens 249446, Peak mem 10.996 GB\n",
      "Iter 350: Train loss 0.017, Learning Rate 1.000e-05, It/sec 0.288, Tokens/sec 211.414, Trained Tokens 256790, Peak mem 10.996 GB\n",
      "Iter 360: Train loss 0.017, Learning Rate 1.000e-05, It/sec 0.301, Tokens/sec 227.986, Trained Tokens 264360, Peak mem 10.996 GB\n",
      "Iter 370: Train loss 0.017, Learning Rate 1.000e-05, It/sec 0.328, Tokens/sec 233.481, Trained Tokens 271476, Peak mem 10.996 GB\n",
      "Iter 380: Train loss 0.017, Learning Rate 1.000e-05, It/sec 0.332, Tokens/sec 243.979, Trained Tokens 278820, Peak mem 10.996 GB\n",
      "Iter 390: Train loss 0.016, Learning Rate 1.000e-05, It/sec 0.329, Tokens/sec 249.035, Trained Tokens 286390, Peak mem 10.996 GB\n",
      "Iter 400: Val loss 0.015, Val took 5.119s\n",
      "Iter 400: Train loss 0.017, Learning Rate 1.000e-05, It/sec 4.729, Tokens/sec 3365.007, Trained Tokens 293506, Peak mem 10.996 GB\n",
      "Iter 400: Saved adapter weights to ../Models/FineTune/8b-Instruct/Adapters/adapters.safetensors and ../Models/FineTune/8b-Instruct/Adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.016, Learning Rate 1.000e-05, It/sec 0.321, Tokens/sec 235.703, Trained Tokens 300850, Peak mem 10.996 GB\n",
      "Iter 420: Train loss 0.016, Learning Rate 1.000e-05, It/sec 0.315, Tokens/sec 238.778, Trained Tokens 308420, Peak mem 10.996 GB\n",
      "Iter 430: Train loss 0.016, Learning Rate 1.000e-05, It/sec 0.298, Tokens/sec 225.807, Trained Tokens 315990, Peak mem 10.996 GB\n",
      "Iter 440: Train loss 0.017, Learning Rate 1.000e-05, It/sec 0.305, Tokens/sec 210.074, Trained Tokens 322880, Peak mem 10.996 GB\n",
      "Iter 450: Train loss 0.016, Learning Rate 1.000e-05, It/sec 0.261, Tokens/sec 197.478, Trained Tokens 330450, Peak mem 10.996 GB\n",
      "Iter 460: Train loss 0.016, Learning Rate 1.000e-05, It/sec 0.268, Tokens/sec 196.478, Trained Tokens 337794, Peak mem 10.996 GB\n",
      "Iter 470: Train loss 0.017, Learning Rate 1.000e-05, It/sec 0.290, Tokens/sec 206.692, Trained Tokens 344910, Peak mem 10.996 GB\n",
      "Iter 480: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.282, Tokens/sec 213.799, Trained Tokens 352480, Peak mem 10.996 GB\n",
      "Iter 490: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.290, Tokens/sec 219.845, Trained Tokens 360050, Peak mem 10.996 GB\n",
      "Iter 500: Val loss 0.015, Val took 5.534s\n",
      "Iter 500: Train loss 0.016, Learning Rate 1.000e-05, It/sec 4.374, Tokens/sec 3014.024, Trained Tokens 366940, Peak mem 10.996 GB\n",
      "Iter 500: Saved adapter weights to ../Models/FineTune/8b-Instruct/Adapters/adapters.safetensors and ../Models/FineTune/8b-Instruct/Adapters/0000500_adapters.safetensors.\n",
      "Saved final adapter weights to ../Models/FineTune/8b-Instruct/Adapters/adapters.safetensors.\n",
      "Finished FineTuning\n",
      "Duration: 0:31:25.578263\n"
     ]
    }
   ],
   "source": [
    "print('Starting FineTune')\n",
    "start_time = datetime.now()\n",
    "\n",
    "run = f\"python3 -m mlx_lm.lora --model {mlxModelPath} --data {dataPath} --train --iters {iterations} --max-seq-length {max_length} --adapter-path {adapter_path}\"\n",
    "print(run)\n",
    "!{run}\n",
    "\n",
    "print('Finished FineTuning')\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a2d764-8971-4fcd-8f08-0a49807339f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output FineTune\n",
      "python3 -m mlx_lm.fuse --model ../Models/hf_converted_models/8b-Instruct --save-path ../Models/mlx_models/8b-Instruct-FineTuned --adapter-path ../Models/FineTune/8b-Instruct/Adapters --de-quantize\n",
      "/Users/##USER##/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Loading pretrained model\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "De-quantizing model\n",
      "Finished Output FineTune\n"
     ]
    }
   ],
   "source": [
    "print('Output FineTune')\n",
    "\n",
    "#lora(model=mlxModelPath, train=True, data=dataPath, iters 600)\n",
    "run = f\"python3 -m mlx_lm.fuse --model {hf_converted_model_path} --save-path {newMLXModelPath} --adapter-path {adapter_path} --de-quantize\"\n",
    "print(run)\n",
    "!{run}\n",
    "\n",
    "print('Finished Output FineTune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f15f7b-ee24-485b-8129-c04a2a0fce58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
