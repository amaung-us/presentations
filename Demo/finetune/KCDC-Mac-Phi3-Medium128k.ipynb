{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa2b296-17df-4b27-a307-8b0c2d6b13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ad8e79-9279-411e-91a2-173aff02c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlxModelPath = \"../Models/mlx_models/Phi-3-medium-128k-instruct\"\n",
    "hf_converted_model_path=\"../Models/Original/Phi-3-medium-128k-instruct\"\n",
    "\n",
    "newMLXModelPath = \"../Models/mlx_models/Phi-3-medium-128k-instruct-FineTuned\"\n",
    "dataPath=\"../Data/Jsonl\"\n",
    "iterations=500\n",
    "max_length=2049\n",
    "adapter_path=\"../Models/FineTune/Phi-3-medium-128k-instruct/Adapters\"\n",
    "savePath = \"../Models/saved-model/kcdc-phi3-128k\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ac4840-53bd-413a-9db2-bb10e396927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import lora, generate, load, stream_generate\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef792f7f-c290-44cb-bc83-0d40a1817874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FineTune\n",
      "python3 -m mlx_lm.lora --model ../Models/mlx_models/Phi-3-medium-128k-instruct --data ../Data/Jsonl --train --iters 500 --max-seq-length 2049 --adapter-path ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters\n",
      "/Users/##USER##/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Loading pretrained model\n",
      "[WARNING] rope_scaling 'type' currently only supports 'linear' setting rope scaling to false.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.012% (1.638M/13960.238M)\n",
      "Starting training..., iters: 500\n",
      "Iter 1: Val loss 2.224, Val took 9.071s\n",
      "Iter 10: Train loss 2.303, Learning Rate 1.000e-05, It/sec 0.213, Tokens/sec 191.043, Trained Tokens 8951, Peak mem 32.195 GB\n",
      "Iter 20: Train loss 2.463, Learning Rate 1.000e-05, It/sec 0.220, Tokens/sec 176.454, Trained Tokens 16989, Peak mem 32.195 GB\n",
      "Iter 30: Train loss 2.131, Learning Rate 1.000e-05, It/sec 0.204, Tokens/sec 182.565, Trained Tokens 25940, Peak mem 32.195 GB\n",
      "Iter 40: Train loss 2.018, Learning Rate 1.000e-05, It/sec 0.200, Tokens/sec 179.221, Trained Tokens 34891, Peak mem 32.195 GB\n",
      "Iter 50: Train loss 1.968, Learning Rate 1.000e-05, It/sec 0.126, Tokens/sec 109.266, Trained Tokens 43532, Peak mem 32.195 GB\n",
      "Iter 60: Train loss 1.820, Learning Rate 1.000e-05, It/sec 0.093, Tokens/sec 77.681, Trained Tokens 51880, Peak mem 32.195 GB\n",
      "Iter 70: Train loss 1.563, Learning Rate 1.000e-05, It/sec 0.121, Tokens/sec 100.633, Trained Tokens 60228, Peak mem 32.195 GB\n",
      "Iter 80: Train loss 1.229, Learning Rate 1.000e-05, It/sec 0.127, Tokens/sec 117.099, Trained Tokens 69472, Peak mem 32.195 GB\n",
      "Iter 90: Train loss 0.959, Learning Rate 1.000e-05, It/sec 0.188, Tokens/sec 156.889, Trained Tokens 77820, Peak mem 32.195 GB\n",
      "Iter 100: Train loss 0.653, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 165.870, Trained Tokens 86771, Peak mem 32.195 GB\n",
      "Iter 100: Saved adapter weights to ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters/adapters.safetensors and ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 0.447, Learning Rate 1.000e-05, It/sec 0.207, Tokens/sec 166.564, Trained Tokens 94809, Peak mem 32.195 GB\n",
      "Iter 120: Train loss 0.268, Learning Rate 1.000e-05, It/sec 0.180, Tokens/sec 160.858, Trained Tokens 103760, Peak mem 32.195 GB\n",
      "Iter 130: Train loss 0.175, Learning Rate 1.000e-05, It/sec 0.158, Tokens/sec 141.445, Trained Tokens 112711, Peak mem 32.195 GB\n",
      "Iter 140: Train loss 0.126, Learning Rate 1.000e-05, It/sec 0.150, Tokens/sec 129.691, Trained Tokens 121352, Peak mem 32.195 GB\n",
      "Iter 150: Train loss 0.069, Learning Rate 1.000e-05, It/sec 0.165, Tokens/sec 137.959, Trained Tokens 129700, Peak mem 32.195 GB\n",
      "Iter 160: Train loss 0.046, Learning Rate 1.000e-05, It/sec 0.178, Tokens/sec 148.195, Trained Tokens 138048, Peak mem 32.195 GB\n",
      "Iter 170: Train loss 0.035, Learning Rate 1.000e-05, It/sec 0.170, Tokens/sec 157.016, Trained Tokens 147292, Peak mem 32.195 GB\n",
      "Iter 180: Train loss 0.023, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 158.909, Trained Tokens 155640, Peak mem 32.195 GB\n",
      "Iter 190: Train loss 0.020, Learning Rate 1.000e-05, It/sec 0.190, Tokens/sec 164.533, Trained Tokens 164281, Peak mem 32.195 GB\n",
      "Iter 200: Val loss 0.018, Val took 10.773s\n",
      "Iter 200: Train loss 0.018, Learning Rate 1.000e-05, It/sec 1.900, Tokens/sec 1586.363, Trained Tokens 172629, Peak mem 32.195 GB\n",
      "Iter 200: Saved adapter weights to ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters/adapters.safetensors and ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 0.017, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 167.461, Trained Tokens 181580, Peak mem 32.195 GB\n",
      "Iter 220: Train loss 0.017, Learning Rate 1.000e-05, It/sec 0.188, Tokens/sec 162.800, Trained Tokens 190221, Peak mem 32.195 GB\n",
      "Iter 230: Train loss 0.017, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 155.650, Trained Tokens 198569, Peak mem 32.195 GB\n",
      "Iter 240: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.173, Tokens/sec 154.519, Trained Tokens 207520, Peak mem 32.195 GB\n",
      "Iter 250: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 157.848, Trained Tokens 216161, Peak mem 32.195 GB\n",
      "Iter 260: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 161.451, Trained Tokens 224819, Peak mem 32.195 GB\n",
      "Iter 270: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.185, Tokens/sec 159.671, Trained Tokens 233460, Peak mem 32.195 GB\n",
      "Iter 280: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.179, Tokens/sec 160.260, Trained Tokens 242411, Peak mem 32.195 GB\n",
      "Iter 290: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.187, Tokens/sec 155.798, Trained Tokens 250759, Peak mem 32.195 GB\n",
      "Iter 300: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.179, Tokens/sec 154.290, Trained Tokens 259400, Peak mem 32.195 GB\n",
      "Iter 300: Saved adapter weights to ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters/adapters.safetensors and ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.186, Tokens/sec 155.568, Trained Tokens 267748, Peak mem 32.195 GB\n",
      "Iter 320: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.183, Tokens/sec 169.168, Trained Tokens 276992, Peak mem 32.195 GB\n",
      "Iter 330: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.210, Tokens/sec 175.476, Trained Tokens 285340, Peak mem 32.195 GB\n",
      "Iter 340: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.194, Tokens/sec 161.740, Trained Tokens 293688, Peak mem 32.195 GB\n",
      "Iter 350: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.103, Tokens/sec 89.257, Trained Tokens 302329, Peak mem 32.195 GB\n",
      "Iter 360: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.097, Tokens/sec 87.255, Trained Tokens 311280, Peak mem 32.195 GB\n",
      "Iter 370: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.169, Tokens/sec 140.992, Trained Tokens 319628, Peak mem 32.195 GB\n",
      "Iter 380: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.184, Tokens/sec 158.886, Trained Tokens 328269, Peak mem 32.195 GB\n",
      "Iter 390: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.191, Tokens/sec 170.937, Trained Tokens 337220, Peak mem 32.195 GB\n",
      "Iter 400: Val loss 0.013, Val took 10.444s\n",
      "Iter 400: Train loss 0.014, Learning Rate 1.000e-05, It/sec 2.870, Tokens/sec 2395.687, Trained Tokens 345568, Peak mem 32.195 GB\n",
      "Iter 400: Saved adapter weights to ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters/adapters.safetensors and ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.196, Tokens/sec 169.485, Trained Tokens 354209, Peak mem 32.195 GB\n",
      "Iter 420: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.167, Tokens/sec 149.298, Trained Tokens 363160, Peak mem 32.195 GB\n",
      "Iter 430: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.130, Tokens/sec 115.963, Trained Tokens 372111, Peak mem 32.195 GB\n",
      "Iter 440: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.156, Tokens/sec 125.280, Trained Tokens 380149, Peak mem 32.195 GB\n",
      "Iter 450: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.164, Tokens/sec 146.671, Trained Tokens 389100, Peak mem 32.195 GB\n",
      "Iter 460: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.182, Tokens/sec 157.444, Trained Tokens 397741, Peak mem 32.195 GB\n",
      "Iter 470: Train loss 0.015, Learning Rate 1.000e-05, It/sec 0.198, Tokens/sec 165.339, Trained Tokens 406089, Peak mem 32.195 GB\n",
      "Iter 480: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.193, Tokens/sec 172.698, Trained Tokens 415040, Peak mem 32.195 GB\n",
      "Iter 490: Train loss 0.014, Learning Rate 1.000e-05, It/sec 0.192, Tokens/sec 171.879, Trained Tokens 423991, Peak mem 32.195 GB\n",
      "Iter 500: Val loss 0.013, Val took 11.298s\n",
      "Iter 500: Train loss 0.014, Learning Rate 1.000e-05, It/sec 2.647, Tokens/sec 2127.455, Trained Tokens 432029, Peak mem 32.195 GB\n",
      "Iter 500: Saved adapter weights to ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters/adapters.safetensors and ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters/0000500_adapters.safetensors.\n",
      "Saved final adapter weights to ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters/adapters.safetensors.\n",
      "Finished FineTuning\n",
      "Duration: 0:50:05.478057\n"
     ]
    }
   ],
   "source": [
    "print('Starting FineTune')\n",
    "start_time = datetime.now()\n",
    "run = f\"python3 -m mlx_lm.lora --model {mlxModelPath} --data {dataPath} --train --iters {iterations} --max-seq-length {max_length} --adapter-path {adapter_path}\"\n",
    "print(run)\n",
    "!{run}\n",
    "\n",
    "print('Finished FineTuning')\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a2d764-8971-4fcd-8f08-0a49807339f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output FineTune\n",
      "python3 -m mlx_lm.fuse --model ../Models/Original/Phi-3-medium-128k-instruct --save-path ../Models/mlx_models/Phi-3-medium-128k-instruct-FineTuned --adapter-path ../Models/FineTune/Phi-3-medium-128k-instruct/Adapters --de-quantize --save-path ../Models/saved-model/kcdc-phi3-128k\n",
      "/Users/##USER##/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Loading pretrained model\n",
      "[WARNING] rope_scaling 'type' currently only supports 'linear' setting rope scaling to false.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "De-quantizing model\n",
      "Finished Fusing\n"
     ]
    }
   ],
   "source": [
    "print('Output FineTune')\n",
    "\n",
    "#lora(model=mlxModelPath, train=True, data=dataPath, iters 600)\n",
    "run = f\"python3 -m mlx_lm.fuse --model {hf_converted_model_path} --adapter-path {adapter_path} --de-quantize --save-path {savePath}\"\n",
    "print(run)\n",
    "!{run}\n",
    "\n",
    "print('Finished Fusing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4cc23-8cd8-4c25-8a63-d9e33905c716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
