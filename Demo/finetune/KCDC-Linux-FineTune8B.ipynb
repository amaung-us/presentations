{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1916ac8c-0d40-46c2-a528-8b29ca1b4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90bc87d-6755-436f-8e4d-a81d0b5be9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    LlamaForCausalLM\n",
    ")\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"  #Disabling for dual GPU for RTX4090s\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"   #Disabling for dual GPU for RTX4090s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10324b0d-3247-4dd3-bad2-4c07a7229e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"../Models/hf_converted_models/Meta-llama-3-8B-Instruct\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"../Data/Jsonl/train.jsonl\"\n",
    "test_dataset_name= \"../Data/Jsonl/test.jsonl\"\n",
    "# Fine-tuned model name\n",
    "new_model = \"../Models/FineTune/Meta-llama-3-8B-Instruct-finetuned-KCDC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21893eba-acf5-4f44-b4a3-5b42961bc27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16 \n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 500\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 6\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None #will default to 1024\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036aeac4-1602-474b-becf-6e58332456d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset('json', data_files={ 'train': [dataset_name], 'test':test_dataset_name}, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e40f62a-520a-4d7a-a91b-12bca5d01a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3141f5ae3344bebd4e284734865baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 21:10, Epoch 500/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.077300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.900700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.013900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Duration: 0:21:12.257754\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "    ddp_find_unused_parameters=False,\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "start_train_time = datetime.now()\n",
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "end_train_time = datetime.now()\n",
    "print('Train Duration: {}'.format(end_train_time - start_train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0430936-9725-493e-8fc3-a441d12af4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f8b2e16b6246f38c24441cc256d360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    new_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d1a095-e63e-45e7-8ebb-cce6c8cf8164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a new presentation on technologies that Min Maung and Lwin Maung normally talk about.\n",
      "Start : 2024-06-27 21:16:43.395527\n",
      "<<SYS>>\n",
      "You are a professional technical public speaker. You write detailed technical abstracts that is engaging and exciting for users.<</SYS>>\n",
      "[INST]Format should be like this: Title: title of the presentation\n",
      "\\Description: 3 paragrahs worth of of content[/INST]\n",
      "[INST]Write a new presentation on technologies that Min Maung and Lwin Maung normally talk about.[/INST]\n",
      "\n",
      "Title: Revolutionizing IoT Systems: Combining AI and Secure Communication Technologies\n",
      "\n",
      "Description: In today's interconnected world, IoT systems have become integral to various industries, from manufacturing to healthcare. However, the sheer scale and complexity of these systems have raised critical security concerns. This presentation explores the powerful combination of Artificial Intelligence (AI) and secure communication technologies in transforming IoT systems into secure and reliable infrastructure for the future. We will delve into the challenges facing IoT systems, such as data theft, device compromise, and system disruption, and introduce innovative solutions leveraging AI and secure communication protocols like TPM 2.0 and DTLS. Join us as we showcase the pivotal role of technology in securing IoT systems and building a safer, more efficient technology ecosystem. \n",
      "\n",
      "Key takeaways will include understanding the challenges in IoT security, introduction to AI in IoT security, secure communication protocols like TPM 2.0 and DTLS, and their intersection in creating a secure and reliable IoT environment. This presentation will appeal to all technicians, engineers, and business leaders seeking to revolutionize their IoT systems with cutting-edge technology and unwavering security.  Join us on this journey into the future of IoT security.  [Min Maung and Lwin Maung] are the pioneers of this technology and have worked closely with leading organizations to develop this vision for the world.  Contact us to present at your organization or conference.  Email: [min.maung@secureiot.com](mailto:min.maung@secureiot.com) or [lwin.maung@secureiot.com](mailto:lwin.maung@secureiot.com)  Thank you.  [SYS]>>  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [INST]  [\n",
      "Inference Duration: 0:01:41.590928\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "start_time = datetime.now()\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Write a new presentation on technologies that Min Maung and Lwin Maung normally talk about.\"\n",
    "context = \"\"\"\n",
    "You are a professional technical public speaker. You write detailed technical abstracts that is engaging and exciting for users.\"\"\"\n",
    "final_prompt = f\"\"\"<<SYS>>{context}<</SYS>>\n",
    "[INST]Format should be like this: Title: title of the presentation\\n\\Description: 3 paragrahs worth of of content[/INST]\n",
    "[INST]{prompt}[/INST]\\n\\n\"\"\"\n",
    "\n",
    "print(prompt)\n",
    "print('Start : {}'.format(datetime.now()))\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2048,top_p=.7,temperature=0.3)\n",
    "result = pipe(final_prompt)\n",
    "print(result[0]['generated_text'])\n",
    "end_time = datetime.now()\n",
    "print('Inference Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b1300-8a9a-4aca-9505-d469ad297062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
