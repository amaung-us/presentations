{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762dd02-1fda-4c80-8633-00597b7c9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc59722-e918-4298-a0e0-128ba1ba79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import references\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, logging, pipeline, LlamaForCausalLM\n",
    "\n",
    "from threading import Thread\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    LogitsProcessorList,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    "    AutoTokenizer, \n",
    "    logging, \n",
    "    pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    TextStreamer,\n",
    "    TextIteratorStreamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7669e-c493-4eb0-836f-65591094f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"../Models/hf_converted_models/Meta-llama-3-8B-Instruct\"\n",
    "fine_tuned_model = \"../Models/FineTune/Meta-llama-3-8B-Instruct-finetuned-KCDC\"\n",
    "#fine_tuned_model = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4a724-7a78-49eb-aa36-d22b7928f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    fine_tuned_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb179e-9814-4568-9692-86e5d0dfd907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"Write a new presentation on technologies that Min Maung and Lwin Maung normally talk about.\"\n",
    "print(prompt)\n",
    "\n",
    "gpu_start_time = datetime.now()\n",
    "print('Start : {}'.format(gpu_start_time))\n",
    "pipe = pipeline(task=\"summarization\", model=model, tokenizer=tokenizer, max_length=1024)\n",
    "result = pipe(prompt)\n",
    "print(result)\n",
    "gpu_end_time = datetime.now()\n",
    "print('End : {}'.format(gpu_end_time))\n",
    "print('GPU Inference Duration: {}\\n'.format(gpu_end_time - gpu_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6aaf0-02be-4799-8d9b-ffb02148d8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
